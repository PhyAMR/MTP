% !TeX encoding = UTF-8
% Lecture file created by newnote
% Class: Models of Theoretical Physics
% Professor: Azaele Sandro
% Date: 2025-10-03
\lecture{11}{Gaussian Integrals continued}{2025-10-03}
\pagelayout{margin}
% --- Start writing here ---

\section{Gaussian Integrals continued}
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} e^{-\frac{a x^{2}}{2}+b x} d x=\sqrt{\frac{2 \pi}{a}} e^{\frac{b^{2}}{2 a}} \quad a>0, b \in \mathbb{C}
\end{DispWithArrows}
\begin{DispWithArrows}[displaystyle, format=c]
  \varphi(k)=\int e^{i k x} p(x) d x=\left\langle e^{i k x}\right\rangle
\end{DispWithArrows}
\begin{DispWithArrows}[displaystyle, format=c]
  \left.(-i)^{n} \frac{d^{n} \varphi}{d k^{n}}\right|_{k=0}=\left\langle x^{n}\right\rangle
\end{DispWithArrows}
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{x^{2}}{2 \sigma^{2}}} e^{i k x} d x=e^{-\frac{\sigma^{2} k^{2}}{2}}
\end{DispWithArrows}
Because of eq. (5)
\begin{DispWithArrows}[displaystyle, format=c]
  \langle x\rangle=-\left.i \frac{d}{d k} e^{-\frac{\sigma^{2} k^{2}}{2}}\right|_{k=0}=0
\end{DispWithArrows}
Important Gaussian integrals:
$\left\langle x^{n}\right\rangle=\left.(-i)^{n} \frac{d^{n}}{d k^{n}} e^{-\frac{\sigma^{2} k^{2}}{2}}\right|_{k=0}=0$
if $n$ is odd (by symmetry).
However
\begin{DispWithArrows}[displaystyle, format=c]
  \left\langle x^{4}\right\rangle=\left.\frac{d^{4}}{d k^{4}} e^{-\frac{\sigma^{2} k^{2}}{2}}\right|_{k=0}=\left[\sigma^{4}\left(3-6 k^{2} \sigma^{2}+k^{4} \sigma^{4}\right) e^{-\frac{\sigma^{2} k^{2}}{2}}\right]_{k=0} = 3 \sigma^{4}=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \int_{-\infty}^{+\infty} e^{-\frac{x^{2}}{2 \sigma^{2}}} x^{4} d x .
\end{DispWithArrows}
If we want to calculate $\left\langle x^{n}\right\rangle$, we better start from
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} e^{-\frac{a x^{2}}{2}} d x=\sqrt{\frac{2 \pi}{a}}
\end{DispWithArrows}
We differentiate both sides wrt a:
once
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} x^{2} e^{-\frac{a x^{2}}{2}} d x=\frac{\sqrt{2 \pi}}{a^{3 / 2}} \rightarrow\left\langle x^{2}\right\rangle
\end{DispWithArrows}
twice
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} x^{4} e^{-\frac{a x^{2}}{2}} d x=\frac{3 \sqrt{2 \pi}}{a^{5 / 2}}
\end{DispWithArrows}
$n/2$ times, $n$ is even
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} x^{n} e^{-\frac{a x^{2}}{2}} d x=\frac{(n-1)!! \sqrt{2 \pi}}{a^{(n+1) / 2}}
\end{DispWithArrows}
From this find the expression of $\left\langle x^{n}\right\rangle$ as a
function of $\sigma$ and $n$ (even).

\subsection*{Multidimensional Gaussian Integrals}
Example:
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} d x_{1} \int_{-\infty}^{+\infty} d x_{2} e^{-\frac{3}{2}\left(x_{1}^{2}+x_{2}^{2}\right)+x_{1} x_{2}}=?
\end{DispWithArrows}
Ex: write down the exponent in the form
$-\frac{1}{2} \vec{x}^{\top} A \vec{x}$.
More generally,
\begin{DispWithArrows}[displaystyle, format=c]
  Z(A)=\int_{\mathbb{R}^{m}} d^{m}x e^{-\frac{1}{2} \vec{x}^{\top} A \vec{x}}
\end{DispWithArrows}
Where $\vec{x}=\left(x_{1}, \ldots, x_{m}\right)$ and the matrix $A$ is
diagonalizable with strictly positive eigenvalues (positive definite). Then
there exist an orthogonal matrix
$O \quad\left(\Rightarrow O O^{\top}=O^{\top} O=\mathbb{1}\right)$ such that we
can define $\vec{y}=O \vec{x}$ and $O A O^{\top}=\Lambda$
\begin{DispWithArrows}[displaystyle, format=c]
  \Lambda=\begin{pmatrix}
    \lambda_{1} & & 0 \\ \nonumber
    & \ddots & \\ \nonumber
    0 & & \lambda_{m}
  \end{pmatrix} \quad \lambda_{i}>0 \quad i=1,2, \ldots m
\end{DispWithArrows}
It follows that
\begin{DispWithArrows}[displaystyle, format=c]
  \vec{x}^{\top} A \vec{x}=\vec{x}^{\top} O^{\top} \Lambda O \vec{x}=\vec{y}^{\top} \Lambda \vec{y}
\end{DispWithArrows}
$Z(A)=\int d^{m}x e^{-\frac{1}{2} \vec{x}^{\top} A \vec{x}}=\int d^{m}y\left|\frac{\partial \vec{x}}{\partial \vec{y}}\right| e^{-\frac{1}{2} \vec{y}^{\top} \Lambda \vec{y}}$
determinant of the Jacobian:
$\left|\frac{\partial \vec{x}}{\partial \vec{y}}\right|=\operatorname{det}\left(O^{\top}\right)=1$
(show as an exercise.)
\begin{DispWithArrows}[displaystyle, format=c]
  \vec{y}^{\top} \Lambda \vec{y}=\sum_{i j} y_{i} \Lambda_{i j} y_{j}=\sum_{i j} y_{i} \lambda_{i} \delta_{i j} y_{j}=\sum_{i} \lambda_{i} y_{i}^{2}
\end{DispWithArrows}
From this we get
\begin{DispWithArrows}[displaystyle, format=c]
  \begin{aligned}
    & =\int_{\mathbb{R}^{m}} d^{m} y e^{-\frac{1}{2} \sum_{i} \lambda_{i} y_{i}^{2}}=\prod_{i=1}^{m} \int_{-\infty}^{+\infty} d y_{i} e^{-\frac{1}{2} \lambda_{i} y_{i}^{2}}=\prod_{i=1}^{m} \sqrt{\frac{2 \pi}{\lambda_{i}}}=\frac{(2 \pi)^{m / 2}}{\sqrt{\lambda_{1} \cdots \lambda_{m}}} \\ \nonumber
    & \operatorname{det}(A)=\operatorname{det}\left(O^{\top} \Lambda O\right)=\operatorname{det}(\Lambda)(\operatorname{det} O)^{2}=\operatorname{det} \Lambda=\lambda_{1} \cdots \lambda_{m}
  \end{aligned}
\end{DispWithArrows}
Hence
\begin{DispWithArrows}[displaystyle, format=c]
  Z(A)=\frac{(2 \pi)^{m / 2}}{\sqrt{\operatorname{det} A}}=\int_{\mathbb{R}^{m}} d^{m}x e^{-\frac{1}{2} \vec{x}^{\top} A \vec{x}}
\end{DispWithArrows}
By using (8) show that
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{-\infty}^{+\infty} d x_{1} \int_{-\infty}^{+\infty} d x_{2} e^{-\frac{3}{2}\left(x_{1}^{2}+x_{2}^{2}\right)+x_{1} x_{2}}=\frac{\pi}{\sqrt{2}}
\end{DispWithArrows}
Exercise: Let
$p(x, y)=\frac{\sqrt{\operatorname{det} A}}{2 \pi} e^{-\frac{1}{2}\left(a_{11} x^{2}+2 a_{12} x y+a_{22} y^{2}\right)}$
where $a_{11}, a_{22}>0$. Show that $q(x)=\int p(x, y) dy$ is still a Gaussian
distribution. Find the corresponding variance of $x$. This is also true for
m-dimension Gaussian variables.

We want now to calculate
\begin{DispWithArrows}[displaystyle, format=c]
  Z(A, \vec{b})=\int d^{m} x e^{-\frac{1}{2} \vec{x}^{\top} A \vec{x}+\vec{x}^{\top} \cdot \vec{b}}
\end{DispWithArrows}
we use the same strategy we used before:
\begin{DispWithArrows}[displaystyle, format=c]
  \vec{\nabla}_{x}\left(-\frac{1}{2} \vec{x}^{\top} A \vec{x}+\vec{x}^{\top} \cdot \vec{b}\right)=-A \vec{x}+\vec{b}=0 \Rightarrow \vec{x}=A^{-1} \vec{b} \quad (\text{if }\det A \neq 0)
\end{DispWithArrows}
We introduce
\begin{DispWithArrows}[displaystyle, format=ll]
  \begin{aligned}
    \vec{y} & =\vec{x}-A^{-1} \vec{b} \\ \nonumber
    -\frac{1}{2} \vec{x}^{\top} A \vec{x}+\vec{x}^{\top} \cdot \vec{b} & =-\frac{1}{2} \vec{y}^{\top} A \vec{y}+\frac{\vec{b}^{\top} A^{-1} \vec{b}}{2} \quad \text { (do the calculation). }
  \end{aligned}
\end{DispWithArrows}
Hence
\begin{DispWithArrows}[displaystyle, format=c]
  Z(A, \vec{b})=\int_{\mathbb{R}^{m}} d^{m}y e^{-\frac{1}{2} \vec{y}^{\top} A \vec{y}+\frac{\vec{b}^{\top} A^{-1} \vec{b}}{2}}=e^{\frac{\vec{b}^{\top} A^{-1} \vec{b}}{2}} Z(A, 0)
\end{DispWithArrows}
\begin{DispWithArrows}[displaystyle, format=c]
  Z(A, \vec{b})=\frac{(2 \pi)^{m / 2}}{\sqrt{\operatorname{det} A}} e^{\frac{\vec{b}^{\top} A^{-1} \vec{b}}{2}}
\end{DispWithArrows}
Eq. (10) allows to find the charact. function of the multivariate Gaussian
distrib.
\begin{DispWithArrows}[displaystyle, format=c]
  p(\vec{x})=\frac{1}{Z(A, 0)} e^{-\frac{1}{2} \vec{x}^{\top} A \vec{x}}
\end{DispWithArrows}
\begin{DispWithArrows}[displaystyle, format=c]
  \int_{\mathbb{R}^{m}} d^{m}x p(x) e^{i \vec{k} \cdot \vec{x}}=e^{-\frac{\vec{k}^{\top} A^{-1} \vec{k}}{2}}=\varphi(k)
\end{DispWithArrows}
What is the meaning of $A^{-1}$?
The definition of the ch. f. in the multidim. case is:
\begin{DispWithArrows}[displaystyle, format=c]
  \varphi(\vec{k})=\int d^{m}x e^{i \vec{k} \cdot \vec{x}} p(\vec{x}) \quad \vec{k}=\left(k_{1}, k_{2}, \ldots, k_{m}\right)
\end{DispWithArrows}
therefore we derive
\begin{DispWithArrows}[displaystyle, format=c]
  \left.(-i)^{s} \frac{\partial^s}{\partial k_{i} \partial k_{j} \cdots \partial k_{l}} \varphi(\vec{k})\right|_{\vec{k}=0}=\int d^{m} x x_{i} x_{j} \cdots x_{l} p(\vec{x})=\left\langle x_{i} x_{j} \cdots x_{l}\right\rangle
\end{DispWithArrows}
Let's calculate the 2-point correlation function for a Gaussian distr.
\begin{DispWithArrows}[displaystyle, format=c]
  \left\langle x_{i} x_{j}\right\rangle=\left.(-i)^{2} \frac{\partial}{\partial k_{i}} \frac{\partial}{\partial k_{j}} e^{-\frac{\vec{k}^{\top} A^{-1} \vec{k}}{2}}\right|_{\vec{k}=0}=\left(A^{-1}\right)_{i j}
\end{DispWithArrows}
$A^{-1}$ is the 2-point correlation function between a pair of Gauss. r.v. When
$A^{-1}$ is a diagonal matrix, we say that the vars are uncorrelated.

In the previous example:
$A=\begin{pmatrix}3 & -1 \\ -1 & 3\end{pmatrix} \quad A^{-1}=\frac{1}{8}\begin{pmatrix}3 & 1 \\ 1 & 3\end{pmatrix}$
hence
\begin{DispWithArrows}[displaystyle, format=c]
  \left\langle x_{1}^{2}\right\rangle=\frac{3}{8}=\left\langle x_{2}^{2}\right\rangle \quad\left\langle x_{1} x_{2}\right\rangle=\frac{1}{8}=\left\langle x_{2} x_{1}\right\rangle
\end{DispWithArrows}
Notice that, because of symmetry, the s-point correl. funct. for s-variable
($s$ is odd) is zero (the Gaussian remains unchanged when
$\vec{x} \rightarrow-\vec{x}$).

What happens when we calculate
$\left\langle x_{i} x_{j} \ldots x_{l}\right\rangle$?
Should we do all the derivatives like in eq. (12)? No!
If the vars are Gaussian then we can use:

\subsection*{Wick's Theorem}
Any correlation between an even number of zero-mean Gaussian r.v. can be
written down as a sum of products of 2-point correlation functions
$\left(A^{-1}\right)$.
For instance:
$\left\langle x_{a} x_{b} x_{c} x_{d}\right\rangle=\left\langle x_{a} x_{b}\right\rangle\left\langle x_{c} x_{d}\right\rangle+\left\langle x_{a} x_{c}\right\rangle\left\langle x_{b} x_{d}\right\rangle+\left\langle x_{a} x_{d}\right\rangle\left\langle x_{b} x_{c}\right\rangle$
In general
\begin{DispWithArrows}[displaystyle, format=c]
  \langle\underbrace{x_{i} x_{j} \cdots x_{m} x_{n}}_{s \text { vars }}\rangle=\sum_{p}\left(A^{-1}\right)_{i_{p} j_{p}} \cdots\left(A^{-1}\right)_{m_{p} n_{p}}
\end{DispWithArrows}
where the sum is over all possible pairings of $s$ indexes, i.e. over all ways
of grouping $s$ (even) indexes $i, j \ldots, m, n$ into pairs (counting pairs
even when indexes are equal).
Exercise: show that
\begin{DispWithArrows}[displaystyle, format=ll]
  \begin{aligned}
    & \left\langle x_{1}^{2} x_{2}^{2}\right\rangle=\left\langle x_1 x_1\right\rangle \left\langle x_2 x_2\right\rangle + 2\left\langle x_1 x_2\right\rangle^2=\frac{3}{8} \cdot \frac{3}{8}+2\left(\frac{1}{8}\right)^2=\frac{11}{64} \\ \nonumber
    & \left\langle x_{1}^{4}\right\rangle=3\left\langle x_1^2\right\rangle^2=3\left(\frac{3}{8}\right)^{2}=\frac{27}{64}
  \end{aligned}
\end{DispWithArrows}
J. Zinn-Justin, Quantum Field Theory and Critical Phenomena (ch. 1).

\subsection*{Important results obtained with characteristic functions}
If we are given the joint probability density function $p(x_{1}, x_{2})$ and it
happens that $p(x_{1}, x_{2})=p_{1}(x_{1}) p_{2}(x_{2})$ then the two vars are
independent. If, on top of this, $p_{1}=p_{2}$, then we say that the two r.v.
are independent and identically distributed (i.i.d.).

If we are given two r.v. that are i.i.d. can we calculate the distribution of
their sum?
If $x_{1} \sim q(x)$ and $x_{2} \sim q(x)$ what is the distribution of
$x=x_{1}+x_{2}$?
